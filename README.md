# KleptoSyn

Synthetic data generation for investigative graphs based on patterns
of bad-actor tradecraft.

Default input data sources:

  * <https://www.opensanctions.org/>
  * <https://www.openownership.org/>
  * <https://www.occrp.org/en/project/the-azerbaijani-laundromat/the-raw-data>

Ontologies used:

  * <https://followthemoney.tech/>

The simulation uses the following process:

  1. Construct a _Network_ that represents bad-actor subgraphs

     * Use `OpenSanctions` (risk data) and `Open Ownership` (link data) for real-world UBO topologies
     * Run `Senzing` entity resolution to generate a "backbone" for organizing the graph
     * Partition into subgraphs and run centrality measures to identify UBO owners

  2. Configure a _Simulation_ for generating patterns of bad-actor tradecraft

     * Analyze the transactions of the OCCRP "Azerbaijani Laundromat" leaked dataset (event data)
     * Sample probability distributions for shell topologies, transfer amounts, and transfer timing
     * Generate a large portion of "legit" transfers (49:1 ratio)

  3. Generate the _SynData_ (synthetic data) by applying the simulation on the network

     * Track the generated bad-actor transactions
     * Serialize the transactions and people/companies involved

Note that much of the "heavy-lifting" here is _entity resolution_ performed by
[`Senzing`](https://senzing.com/)
and _network analytics_ performed by [`NetworkX`](https://senzing.com/).

As simulations scale, both the data generation and the fraud pattern
detection would benefit by using the
[`cuGraph`](https://github.com/rapidsai/cugraph) high performance
back-end for `NetworkX`.


## build a local environment

This project uses [`poetry`](https://python-poetry.org/docs/basic-usage/)
for dependency management, virtual environment, builds, packaging, etc.
To set up an environment locally:

```bash
git clone https://github.com/DerwenAI/kleptosyn.git
cd kleptosyn

poetry install --extras=demo
```

The source code is currently based on Python 3.11 or later.


## load the default data

```bash
wget https://raw.githubusercontent.com/DerwenAI/senzing_starter_kit/refs/heads/main/senzing_rootfs/data/open-sanctions.json
wget https://raw.githubusercontent.com/DerwenAI/senzing_starter_kit/refs/heads/main/senzing_rootfs/data/open-ownership.json

wget https://storage.googleapis.com/erkg/starterkit/export.json

wget https://raw.githubusercontent.com/cj2001/senzing_occrp_mapping_demo/refs/heads/main/occrp_17k.csv
```

## run the demo script and notebooks

```bash
poetry run python3 demo.py
```

```bash
poetry run jupyter-lab
```


## use the results

By default, the output results will be serialized as:

  + `graph.json`: the network representation
  + `transact.csv`: transactions generated by the simulation
  + `entities.csv`: entities generated by the simulation


## development

First, to set up the `dev` and `test` environment:

```bash
poetry install --extras=dev
poetry install --extras=test
```

This project uses [`pre-commit`](https://pre-commit.com/) hooks for
code linting, etc., whenever `git` is used to commit or push.
Be sure to install *after* the `poetry install` is performed.

To run `pre-commit` explicitly:

```bash
poetry run pre-commit
```
